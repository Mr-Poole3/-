# 第五课作业复现及笔记

## 环境部署

### 安装环境

```python
studio-conda -t lmdeploy -o pytorch-2.1.2
```

![image](https://github.com/Mr-Poole3/InternLM/assets/112788987/81d1c4dc-7de7-4676-a585-ff94664fdcbb)


### 安装0.3版本的lmdeploy。

![image](https://github.com/Mr-Poole3/InternLM/assets/112788987/2de5c0f6-e476-416e-ad4b-47e258f8485d)


### 新建pipeline_transformer.py文件

![image](https://github.com/Mr-Poole3/InternLM/assets/112788987/edee4f1d-035b-400f-96db-eb63802fb12f)


### 测试模型
![image](https://github.com/Mr-Poole3/InternLM/assets/112788987/d026a2ed-1a91-4538-a2a0-8851d9c6ec88)


可以看出，这个推理也是占用了巨大内存

## 使用LMDeploy与模型对话

![image](https://github.com/Mr-Poole3/InternLM/assets/112788987/3b2e0372-9c43-4629-abb8-c699324dfb53)


### 以命令行方式与InternLM2-Chat-1.8B大模型对话

![image](https://github.com/Mr-Poole3/InternLM/assets/112788987/7bb0f295-bdd2-4bdf-a77c-1fc2741bbfcd)


## 笔记
本文介绍了如何在环境中安装和使用LMDeploy进行模型推理。作者通过新建pipeline_transformer.py文件测试模型，发现推理过程占用了大量内存。此外，作者还介绍了如何使用LMDeploy与InternLM2-Chat-1.8B大模型进行命令行对话。
